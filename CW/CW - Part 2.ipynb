{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364c78a3",
   "metadata": {},
   "source": [
    "## Part 2 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d441226",
   "metadata": {},
   "source": [
    "Importing the required libraries and checking for the python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f885b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6070c37",
   "metadata": {},
   "source": [
    "Loading only the training dataset for Part 2 of the coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_DATASET_PATH = \"CW_dataset\"\n",
    "\n",
    "def load_train_data(dataset_path=CW_DATASET_PATH):\n",
    "    x_train_all_path = os.path.join(dataset_path, \"x_train_all.csv\")\n",
    "    y_train_all_path = os.path.join(dataset_path, \"y_train_all.csv\")\n",
    "\n",
    "    x_train_all = pd.read_csv(x_train_all_path)\n",
    "    y_train_all = pd.read_csv(y_train_all_path)\n",
    "\n",
    "    return x_train_all, y_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892a4d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CW_dataset\\\\x_train_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Poornima\\OneDrive - Heriot-Watt University\\Desktop\\dmml portfolio\\DMML-GroupProject\\CW\\CW - Part 2.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train, y_train \u001b[39m=\u001b[39m load_train_data()\n",
      "\u001b[1;32mc:\\Users\\Poornima\\OneDrive - Heriot-Watt University\\Desktop\\dmml portfolio\\DMML-GroupProject\\CW\\CW - Part 2.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x_train_all_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, \u001b[39m\"\u001b[39m\u001b[39mx_train_all.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_train_all_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, \u001b[39m\"\u001b[39m\u001b[39my_train_all.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x_train_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(x_train_all_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_train_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(y_train_all_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Poornima/OneDrive%20-%20Heriot-Watt%20University/Desktop/dmml%20portfolio/DMML-GroupProject/CW/CW%20-%20Part%202.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x_train_all, y_train_all\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Poornima\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CW_dataset\\\\x_train_all.csv'"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6ebcf",
   "metadata": {},
   "source": [
    "### 1) K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9c7d8",
   "metadata": {},
   "source": [
    "We plotted the clusters for our train dataset by using a dimentionality reduction technique called PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1],c=y, s=1) #s is the size of dots.\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca1 = pca.fit_transform(x_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X_pca1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f6f06",
   "metadata": {},
   "source": [
    "We then preprocessed the train dataset and plotted the clusters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ba73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a list to store the labels that have less number of pictures\n",
    "less_class_labels = [0, 5, 6, 7, 8, 9]\n",
    "\n",
    "data_list = []\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]\n",
    "    image_data = row.values.reshape(48, 48)\n",
    "\n",
    "    if label in less_class_labels:\n",
    "        data_list.append((image_data, label))\n",
    "\n",
    "subset_dataset = np.array(data_list, dtype=object)\n",
    "\n",
    "print(subset_dataset.shape)\n",
    "\n",
    "num_images_per_label = {label: np.sum(subset_dataset[:, 1] == label) for label in less_class_labels}\n",
    "\n",
    "for label, num_images in num_images_per_label.items():\n",
    "    print(f\"Label {label}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(feature_vector, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, feature_vector.shape)\n",
    "    noisy_vector = feature_vector + noise\n",
    "    return noisy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69720f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_perturbations(feature_vector, perturbation_level=0.02):\n",
    "    min_value = np.min(feature_vector)\n",
    "    max_value = np.max(feature_vector)\n",
    "    perturbations = np.random.uniform(-perturbation_level, perturbation_level, feature_vector.shape)\n",
    "    perturbed_vector = feature_vector + perturbations * (max_value - min_value)\n",
    "    return perturbed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdac3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "# Winsorize the data (adjust percentiles as needed)\n",
    "winsorized_data = winsorize(x_train, limits=[0.05, 0.05])\n",
    "\n",
    "def normalise(images):\n",
    "    preprocessed_images = []\n",
    "    for image in images: \n",
    "        if(len(image.shape) == 3):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        adjusted = cv2.convertScaleAbs(image, alpha=1.7, beta = 90)\n",
    "        enhanced_image = cv2.equalizeHist(adjusted)\n",
    "        image = enhanced_image/255.0\n",
    "        resize = cv2.resize(image, (48,48))\n",
    "\n",
    "        preprocessed_images.append(resize)\n",
    "    return  np.array(preprocessed_images)\n",
    "\n",
    "\n",
    "x_train = normalise(x_train)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_images = {}\n",
    "\n",
    "num_images_per_label = {}\n",
    "\n",
    "x_train = pd.DataFrame(x_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]  \n",
    "    image_data = row.values.reshape(-1, 48, 48) \n",
    "\n",
    "    if label not in label_images:\n",
    "        label_images[label] = []\n",
    "    \n",
    "    label_images[label].append(image_data)\n",
    "\n",
    "    num_images_per_label[label] = len(label_images[label])\n",
    "\n",
    "box_data = np.array(list(label_images.values()), dtype=object)\n",
    "\n",
    "labels = list(label_images.keys())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(box_data, labels=labels, vert=False)\n",
    "plt.title(\"Box Plots for each label\")\n",
    "plt.xlabel(\"Pixel Values\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.show()\n",
    "\n",
    "for label, num_images in num_images_per_label.items():\n",
    "    print(f\"Label {label}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "augmented_data = []\n",
    "new_augmented_data = []\n",
    "labels_for_train = []\n",
    "for data, label in subset_dataset:\n",
    "    noisy_vector = add_noise(data, noise_level=0.01)  \n",
    "    perturbed_vector = introduce_perturbations(data, perturbation_level=0.02)  \n",
    "    \n",
    "        # Append the original and augmented data with their respective labels\n",
    "    augmented_data.append((data, label))\n",
    "    augmented_data.append((noisy_vector, label))\n",
    "    augmented_data.append((perturbed_vector, label))\n",
    "\n",
    "# Convert the appended data to a NumPy array\n",
    "for image_vectors, label_col in augmented_data:\n",
    "    new_augmented_data.append(image_vectors)\n",
    "    labels_for_train.append(label_col)\n",
    "\n",
    "new_augmented_data = np.array(new_augmented_data)\n",
    "labels_for_train = np.array(labels_for_train)\n",
    "\n",
    "new_labels = labels_for_train.reshape(-1, 1)\n",
    "\n",
    "new_augmented_data = new_augmented_data.reshape(new_augmented_data.shape[0], -1)\n",
    "\n",
    "print(new_augmented_data.shape)\n",
    "print(new_labels.shape)\n",
    "if new_augmented_data.shape[1] == x_train.shape[1]:\n",
    "    x_train = np.vstack((x_train, new_augmented_data))\n",
    "    y_train = np.concatenate((y_train, new_labels))\n",
    "else:\n",
    "    print(\"Number of columns in appended_data doesn't match x_train.\")\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO\n",
    "rotation_angle = 15\n",
    "scale_factor = 1.2\n",
    "\n",
    "rotated_images = []\n",
    "\n",
    "for image in x_train:\n",
    "    image = image.reshape(48, 48)  \n",
    "    rotated_image = cv2.warpAffine(image, cv2.getRotationMatrix2D((image.shape[1] / 2, image.shape[0] / 2), rotation_angle, scale_factor), (image.shape[1], image.shape[0]))\n",
    "    rotated_images.append(rotated_image)\n",
    "\n",
    "x_train = np.asarray(rotated_images)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Additional Pre processing\n",
    "def preprocess_data(data):\n",
    "    preprocessed_images = []\n",
    "    \n",
    "    for image in data:\n",
    "        # Reshaping the image to its original shape\n",
    "        image = image.reshape(48, 48)\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "        \n",
    "        # Apply sharpening filter to enhance image details\n",
    "        sharpened_image = cv2.filter2D(blurred_image, -1, np.array([[-1, -1, -1],\n",
    "                                                                    [-1,  9, -1],\n",
    "                                                                    [-1, -1, -1]]))\n",
    "        \n",
    "        # Standardize pixel values using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        standardized_image = scaler.fit_transform(sharpened_image)\n",
    "        \n",
    "        # Append the preprocessed image to the list\n",
    "        preprocessed_images.append(standardized_image)\n",
    "    \n",
    "    preprocessed_data = np.asarray(preprocessed_images)\n",
    "    \n",
    "    # Reshape the data back to the flattened format\n",
    "    preprocessed_data = preprocessed_data.reshape(preprocessed_data.shape[0], -1)\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "x_train = preprocess_data(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2 = pca.fit_transform(x_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X_pca2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678d2c2",
   "metadata": {},
   "source": [
    "We train a K-Means clusterer on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc39f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(x_train)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plot_clusters(X_pca, cluster_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels is kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = silhouette_score(x_train, cluster_labels)\n",
    "print(\"Silhouette Score = \", silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reapply Standard Scaling and PCA\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# pca = PCA(n_components=0.85)  # Adjusting the variance to 85%\n",
    "# x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "\n",
    "# # 2. t-SNE Dimensionality Reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "x_train_tsne = tsne.fit_transform(x_train_scaled)\n",
    "\n",
    "# Now apply K-means clustering on the t-SNE processed data\n",
    "kmeans_tsne = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "cluster_labels_tsne = kmeans_tsne.fit_predict(x_train_tsne)\n",
    "\n",
    "silhouette_scores_tsne = silhouette_score(x_train_tsne, cluster_labels_tsne)\n",
    "print(\"Silhouette Score with t-SNE + K-Means = \", silhouette_scores_tsne)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plot_clusters(x_train_tsne, cluster_labels_tsne)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Winsorizing Data (if not done already)\n",
    "from scipy.stats import mstats\n",
    "\n",
    "x_train_winsorized = np.apply_along_axis(mstats.winsorize, 0, x_train_scaled, limits=[0.05, 0.05])\n",
    "\n",
    "# 2. Feature Engineering: For demonstration, let's create polynomial features.\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#poly = PolynomialFeatures(2, interaction_only=True)\n",
    "#x_train_poly = poly.fit_transform(x_train_scaled)\n",
    "\n",
    "# 3. Agglomerative Clustering\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# agglo_cluster = AgglomerativeClustering(n_clusters=k)\n",
    "# cluster_labels_agglo = agglo_cluster.fit_predict(x_train_poly)\n",
    "\n",
    "# silhouette_scores_agglo = silhouette_score(x_train_poly, cluster_labels_agglo)\n",
    "# print(\"Silhouette Score with Agglomerative Clustering = \", silhouette_scores_agglo)\n",
    "\n",
    "# 4. t-SNE with different parameters\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=40, learning_rate=300)\n",
    "x_train_tsne = tsne.fit_transform(x_train_winsorized)\n",
    "\n",
    "kmeans_tsne = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "cluster_labels_tsne = kmeans_tsne.fit_predict(x_train_tsne)\n",
    "\n",
    "silhouette_scores_tsne = silhouette_score(x_train_tsne, cluster_labels_tsne)\n",
    "print(\"Silhouette Score with optimized t-SNE + K-Means = \", silhouette_scores_tsne)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plot_clusters(x_train_tsne, cluster_labels_tsne)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
