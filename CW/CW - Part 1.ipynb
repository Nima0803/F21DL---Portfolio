{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364c78a3",
   "metadata": {},
   "source": [
    "## Part 1 - Data Analysis and Bayes Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6ebcf",
   "metadata": {},
   "source": [
    "### 1) Data Visualization and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d441226",
   "metadata": {},
   "source": [
    "Importing the required libraries and checking for the python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db10db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2  # OpenCV Python library for computer vision\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6070c37",
   "metadata": {},
   "source": [
    "Loading only the training set for Part 1 of the coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_DATASET_PATH = \"CW_dataset\"\n",
    "\n",
    "def load_train_data(dataset_path=CW_DATASET_PATH):\n",
    "    x_train_all_path = os.path.join(dataset_path, \"x_train_all.csv\")\n",
    "    y_train_all_path = os.path.join(dataset_path, \"y_train_all.csv\")\n",
    "\n",
    "    x_train_all = pd.read_csv(x_train_all_path)\n",
    "    y_train_all = pd.read_csv(y_train_all_path)\n",
    "\n",
    "    return x_train_all, y_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27ece",
   "metadata": {},
   "source": [
    "Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecaa628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the data\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the first 5 rows of the dataset\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "x_train.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if there are any missing values in the data frame\n",
    "x_train.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aada93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a summary of the dataset using describe\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = y_train['0'].value_counts().sort_index()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8ad66",
   "metadata": {},
   "source": [
    "Visualising the dataset using graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630e4a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='0', data=y_train)\n",
    "plt.title(\"Distribution of Class Labels\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77802396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display one image for each label\n",
    "\n",
    "# Initialize a dictionary to store one image for each label\n",
    "label_images = {}\n",
    "\n",
    "# Iterate through the rows of the DataFrames and find one image for each label\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]  \n",
    "    \n",
    "    if label not in label_images:\n",
    "        # Store the first image for each unique label\n",
    "        label_images[label] = row.values.reshape(48, 48)  \n",
    "    \n",
    "    # Break the loop if we have found one image for each unique label\n",
    "    if len(label_images) == 10:\n",
    "        break\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, (label, image) in enumerate(label_images.items()):\n",
    "    r, c = divmod(i, 5)\n",
    "    axs[r, c].imshow(image)\n",
    "    axs[r, c].set_title(f'Label: {label}')\n",
    "    axs[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155af4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_class_labels = y_train['0'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(unique_class_labels), figsize=(20, 5))\n",
    "\n",
    "for i, class_label in enumerate(unique_class_labels):\n",
    "    # Select an image for each class\n",
    "    class_images = x_train[y_train['0'] == class_label]\n",
    "    \n",
    "    if not class_images.empty:\n",
    "        img = class_images.iloc[0, :-1].values\n",
    "        \n",
    "        # Plot the histogram for the image of each class\n",
    "        axes[i].hist(img, bins=100)\n",
    "        axes[i].set_title(f'Class {class_label}')\n",
    "        axes[i].set_xlabel(\"Pixel Value\")\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(x_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# store image data for each label\n",
    "label_images = {}\n",
    "\n",
    "# just to check if all images are being plotted\n",
    "num_images_per_label = {}\n",
    "\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]  \n",
    "    image_data = row.values.reshape(-1, 48, 48) \n",
    "\n",
    "    if label not in label_images:\n",
    "        label_images[label] = []\n",
    "    \n",
    "    # add image data to associated labels\n",
    "    label_images[label].append(image_data)\n",
    "\n",
    "    # just to check if all images are being plotted\n",
    "    num_images_per_label[label] = len(label_images[label])\n",
    "\n",
    "# get numpy array from dictionary values (image data)\n",
    "box_data = np.array(list(label_images.values()), dtype=object)\n",
    "\n",
    "# labels from keys\n",
    "labels = list(label_images.keys())\n",
    "\n",
    "# box plots for each label \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(box_data, labels=labels, vert=False)\n",
    "plt.title(\"Box Plots for each label\")\n",
    "plt.xlabel(\"Pixel Values\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# just to check if all images are being plotted\n",
    "for label, num_images in num_images_per_label.items():\n",
    "    print(f\"Label {label}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8decd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onevrall_x_train = pd.DataFrame(x_train) \n",
    "no_aug_y_train = pd.DataFrame(y_train)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f6f06",
   "metadata": {},
   "source": [
    "### 2) Preprocessing the Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Categorical Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f64c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a list to store the labels that have less number of pictures\n",
    "less_class_labels = [0, 5, 6, 7, 8, 9]\n",
    "\n",
    "data_list = []\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]\n",
    "    image_data = row.values.reshape(48, 48)\n",
    "\n",
    "    if label in less_class_labels:\n",
    "        data_list.append((image_data, label))\n",
    "\n",
    "subset_dataset = np.array(data_list, dtype=object)\n",
    "\n",
    "print(subset_dataset.shape)\n",
    "\n",
    "num_images_per_label = {label: np.sum(subset_dataset[:, 1] == label) for label in less_class_labels}\n",
    "\n",
    "for label, num_images in num_images_per_label.items():\n",
    "    print(f\"Label {label}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf308f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(feature_vector, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, feature_vector.shape)\n",
    "    noisy_vector = feature_vector + noise\n",
    "    return noisy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_perturbations(feature_vector, perturbation_level=0.02):\n",
    "    min_value = np.min(feature_vector)\n",
    "    max_value = np.max(feature_vector)\n",
    "    perturbations = np.random.uniform(-perturbation_level, perturbation_level, feature_vector.shape)\n",
    "    perturbed_vector = feature_vector + perturbations * (max_value - min_value)\n",
    "    return perturbed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23848b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print('------------------------')\n",
    "\n",
    "augmented_data = []\n",
    "new_augmented_data = []\n",
    "labels_for_train = []\n",
    "for data, label in subset_dataset:\n",
    "    noisy_vector = add_noise(data, noise_level=0.01)  \n",
    "    perturbed_vector = introduce_perturbations(data, perturbation_level=0.02)  \n",
    "    \n",
    "        # Append the original and augmented data with their respective labels\n",
    "    augmented_data.append((data, label))\n",
    "    augmented_data.append((noisy_vector, label))\n",
    "    augmented_data.append((perturbed_vector, label))\n",
    "\n",
    "# Convert the appended data to a NumPy array\n",
    "for image_vectors, label_col in augmented_data:\n",
    "    new_augmented_data.append(image_vectors)\n",
    "    labels_for_train.append(label_col)\n",
    "\n",
    "new_augmented_data = np.array(new_augmented_data)\n",
    "labels_for_train = np.array(labels_for_train)\n",
    "\n",
    "new_labels = labels_for_train.reshape(-1, 1)\n",
    "\n",
    "new_augmented_data = new_augmented_data.reshape(new_augmented_data.shape[0], -1)\n",
    "\n",
    "print(new_augmented_data.shape)\n",
    "print(new_labels.shape)\n",
    "print('------------------------')\n",
    "if new_augmented_data.shape[1] == x_train.shape[1]:\n",
    "    cnb_x_train = np.vstack((x_train, new_augmented_data))\n",
    "    cnb_y_train = np.concatenate((y_train, new_labels))\n",
    "else:\n",
    "    print(\"Number of columns in appended_data doesn't match x_train.\")\n",
    "\n",
    "print(cnb_x_train.shape)\n",
    "print(cnb_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Gaussian Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "\n",
    "def normalise(images):\n",
    "    preprocessed_images = []\n",
    "    for image in images: \n",
    "        if(len(image.shape) == 3):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        adjusted = cv2.convertScaleAbs(image, alpha=1.7, beta = 90)\n",
    "        enhanced_image = cv2.equalizeHist(adjusted)\n",
    "        image = enhanced_image/255.0\n",
    "        resize = cv2.resize(image, (48,48))\n",
    "\n",
    "        preprocessed_images.append(resize)\n",
    "    return  np.array(preprocessed_images)\n",
    "\n",
    "\n",
    "x_train = normalise(x_train)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "augmented_data = []\n",
    "new_augmented_data = []\n",
    "labels_for_train = []\n",
    "for data, label in subset_dataset:\n",
    "    noisy_vector = add_noise(data, noise_level=0.01)  \n",
    "    perturbed_vector = introduce_perturbations(data, perturbation_level=0.02)  \n",
    "    \n",
    "        # Append the original and augmented data with their respective labels\n",
    "    augmented_data.append((data, label))\n",
    "    augmented_data.append((noisy_vector, label))\n",
    "    augmented_data.append((perturbed_vector, label))\n",
    "\n",
    "# Convert the appended data to a NumPy array\n",
    "for image_vectors, label_col in augmented_data:\n",
    "    new_augmented_data.append(image_vectors)\n",
    "    labels_for_train.append(label_col)\n",
    "\n",
    "new_augmented_data = np.array(new_augmented_data)\n",
    "labels_for_train = np.array(labels_for_train)\n",
    "\n",
    "new_labels = labels_for_train.reshape(-1, 1)\n",
    "\n",
    "new_augmented_data = new_augmented_data.reshape(new_augmented_data.shape[0], -1)\n",
    "\n",
    "print(new_augmented_data.shape)\n",
    "print(new_labels.shape)\n",
    "if new_augmented_data.shape[1] == x_train.shape[1]:\n",
    "    x_train = np.vstack((x_train, new_augmented_data))\n",
    "    y_train = np.concatenate((y_train, new_labels))\n",
    "else:\n",
    "    print(\"Number of columns in appended_data doesn't match x_train.\")\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from skimage import io, transform, img_as_ubyte\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# x_train = pd.DataFrame(x_train)\n",
    "# y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# preprocessed_images = []\n",
    "\n",
    "# for index, row in x_train.iterrows():\n",
    "#     label = y_train.iloc[index, 0]\n",
    "\n",
    "    \n",
    "#     # Reshape the image data to its original shape (assuming it's a 48x48 image)\n",
    "#     image = row.values.reshape(48, 48)\n",
    "\n",
    "#     # Define the range of rotation angles (e.g., -10 to 10 degrees)\n",
    "#     min_angle = -10\n",
    "#     max_angle = 10\n",
    "#     rotation_angle = np.random.uniform(min_angle, max_angle)\n",
    "\n",
    "#     # Define a random affine transformation matrix with rotation\n",
    "#     tform = transform.AffineTransform(rotation=np.deg2rad(rotation_angle))\n",
    "\n",
    "#     # Apply the affine transformation to the image\n",
    "#     J2 = transform.warp(image, tform)\n",
    "\n",
    "#     # Convert the transformed image to 8-bit format (if necessary)\n",
    "#     # J2 = img_as_ubyte(J2)\n",
    "\n",
    "#     # Display the original and transformed images\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.title('Original Image')\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.title('Transformed Image')\n",
    "#     plt.imshow(J2)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n",
    "#     for i, (label, image) in enumerate(label_images.items()):\n",
    "#     r, c = divmod(i, 5)\n",
    "#     axs[r, c].imshow(image)\n",
    "#     axs[r, c].set_title(f'Label: {label}')\n",
    "#     axs[r, c].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# x_train_rotated = np.array(preprocessed_images)\n",
    "\n",
    "# # # Reshape the equalized images if necessary\n",
    "# x_train_rotated = x_train_rotated.reshape(x_train_rotated.shape[0], -1)\n",
    "\n",
    "# print(x_train_rotated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30611b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train_rotated\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a57026",
   "metadata": {},
   "source": [
    "### 3) Running Naïve Bayes Classifier on Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2c312",
   "metadata": {},
   "source": [
    "#### 3.1) Categorical Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa64e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "cnb_x_train = np.asarray(cnb_x_train)\n",
    "xtrain_images = cnb_x_train.reshape(cnb_x_train.shape[0], -1)\n",
    "cnb_y_train = np.asarray(cnb_y_train).ravel()\n",
    "clf = CategoricalNB()\n",
    "clf.fit(xtrain_images,cnb_y_train)\n",
    "\n",
    "predictions = clf.predict(cnb_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498c335",
   "metadata": {},
   "source": [
    "#### 3.2) Gaussian Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "xtrain_images = x_train.reshape(x_train.shape[0], -1)\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(xtrain_images, y_train)\n",
    "\n",
    "predictions_gb = clf2.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab84d5",
   "metadata": {},
   "source": [
    "### 4) Evaluation Metrics for the Naïve Bayes Classifier on Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a666808",
   "metadata": {},
   "source": [
    "#### 4.1) Categorical Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(cnb_y_train, predictions)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy\n",
    "accuracy = accuracy_score(cnb_y_train, predictions)\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(cnb_y_train, predictions)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f113d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8)) \n",
    "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True,  fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ac8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix.diagonal()\n",
    "fn = np.sum(confusion_matrix, axis=1) - tp\n",
    "fp = np.sum(confusion_matrix, axis=0) - tp\n",
    "tn = np.sum(confusion_matrix) - (tp + fn + fp)\n",
    "\n",
    "# TP Rate (Sensitivity or Recall)\n",
    "tp_rate = tp / (tp + fn)\n",
    "\n",
    "# FP Rate\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "for class_label, tp, fp in zip(range(len(tp_rate)), tp_rate, fp_rate):\n",
    "    print(f\"Class {class_label}: \\nTP Rate = {tp}, \\nFP Rate = {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588110e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = []\n",
    "unique_labels = np.unique(cnb_y_train)\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    true_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :]) - np.sum(confusion_matrix[:, i]) + confusion_matrix[i, i]\n",
    "    total_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :])\n",
    "    print(f'Label {unique_labels[i]} specificity: {true_negative / total_negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensitivity = recall_score(cnb_y_train, predictions, average=None)\n",
    "for i in range(len(unique_labels)):\n",
    "    print(f'Label {unique_labels[i]} sensitivity: {sensitivity[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a636d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probabilities = clf.predict_proba(cnb_x_train)\n",
    "roc_auc_scores = []\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    fpr, tpr, _ = roc_curve((cnb_y_train == unique_labels[i]).astype(int), probabilities[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdf51d",
   "metadata": {},
   "source": [
    "##### Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had manually created our own cross-validation which divides our dataset into folds and iteratively trains and evaluates our Categorical NB classifier on each fold. We also tried the cross-validation technique of sklearn module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we found that we kept having an error with the cross-validation even though we achieved high accuracy. There were 2 reasons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Our dataset only had Numeric/Continuous data and had no categorical data. We later discovered the actual purpose of a Categorical NB Classifier through this evaluation, that is Categorical NB can only be used categorical data like gender, color, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = pd.DataFrame(cnb_x_train).dtypes\n",
    "numeric_columns = data_types[data_types.apply(lambda x: pd.api.types.is_numeric_dtype(x))].index\n",
    "categorical_columns = data_types[data_types.apply(lambda x: not pd.api.types.is_numeric_dtype(x))].index\n",
    "\n",
    "print(\"Numeric (Continuous) Columns:\")\n",
    "print(numeric_columns)\n",
    "\n",
    "print(\"\\nCategorical Columns:\")\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The 2nd reason is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_features = cnb_x_train.shape[1]\n",
    "num_features_expected = len(clf.n_categories_)\n",
    "list_features = clf.n_categories_\n",
    "print(f\"The CategoricalNB model takes {x_num_features} as input and as shown expects {num_features_expected} features.\")\n",
    "print(f\"But if you look at the actual categories,\", list_features)\n",
    "print(f\"It is converting each of our images into these categories: \", list(set(list_features)))\n",
    "print(\"Due to this, we had an error for cross-validation. And due to this we discovered how categorical NB\")\n",
    "print(\"actually works and why we were having a very good accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3419e",
   "metadata": {},
   "source": [
    "#### 4.2) Gaussian Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_gb = classification_report(y_train, predictions_gb)\n",
    "\n",
    "print(report_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy\n",
    "accuracy_gb = accuracy_score(y_train, predictions_gb)\n",
    "print('Accuracy = ', accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix_gb = confusion_matrix(y_train, predictions_gb)\n",
    "confusion_matrix_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8)) \n",
    "sns.heatmap(confusion_matrix_gb/np.sum(confusion_matrix_gb), annot=True,  fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_gb = confusion_matrix_gb.diagonal()\n",
    "fn_gb = np.sum(confusion_matrix_gb, axis=1) - tp\n",
    "fp_gb = np.sum(confusion_matrix_gb, axis=0) - tp\n",
    "tn_gb = np.sum(confusion_matrix_gb) - (tp + fn + fp)\n",
    "\n",
    "# TP Rate (Sensitivity or Recall)\n",
    "tp_rate_gb = tp_gb / (tp_gb + fn_gb)\n",
    "\n",
    "# FP Rate\n",
    "fp_rate_gb = fp_gb / (fp_gb + tn_gb)\n",
    "\n",
    "for class_label, tp, fp in zip(range(len(tp_rate_gb)), tp_rate_gb, fp_rate_gb):\n",
    "    print(f\"Class {class_label}: \\nTP Rate = {tp}, \\nFP Rate = {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_gb = []\n",
    "unique_labels_gb = np.unique(y_train)\n",
    "\n",
    "for i in range(len(unique_labels_gb)):\n",
    "    true_negative_gb = np.sum(confusion_matrix_gb) - np.sum(confusion_matrix_gb[i, :]) - np.sum(confusion_matrix_gb[:, i]) + confusion_matrix_gb[i, i]\n",
    "    total_negative_gb = np.sum(confusion_matrix_gb) - np.sum(confusion_matrix_gb[i, :])\n",
    "    print(f'Label {unique_labels_gb[i]} specificity: {true_negative_gb / total_negative_gb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensitivity_gb = recall_score(y_train, predictions_gb, average=None)\n",
    "for i in range(len(unique_labels_gb)):\n",
    "    print(f'Label {unique_labels_gb[i]} sensitivity: {sensitivity_gb[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probabilities_gb = clf2.predict_proba(x_train)\n",
    "roc_auc_scores_gb = []\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(unique_labels_gb)):\n",
    "    fpr_gb, tpr_gb, _ = roc_curve((y_train == unique_labels_gb[i]).astype(int), probabilities_gb[:, i])\n",
    "    roc_auc_gb = auc(fpr_gb, tpr_gb)\n",
    "    roc_auc_scores_gb.append(roc_auc_gb)\n",
    "    plt.plot(fpr_gb, tpr_gb, lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_gb))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf2, x_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf2, x_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "clf2_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(clf2_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "seed_value = 24\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, y_train, test_size=0.3, random_state=seed_value)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train_split, y_train_split)\n",
    "\n",
    "predicted = gnb.predict(x_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78832353",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test_split, predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test_split, predicted, average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test_split, predicted)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b157dc",
   "metadata": {},
   "source": [
    "### 5) Top Correlating Features - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defined the number of features to select for each class\n",
    "no_of_features_per_class = [5, 10, 20]\n",
    "\n",
    "# Dictionary stores the top features for each class and each dataset\n",
    "top_features_per_cd = {}\n",
    "\n",
    "x_train_class_vs_rest = onevrall_x_train\n",
    "\n",
    "for class_label in range(10):\n",
    "    onevrsall_path = os.path.join(\"OnevrsAll\", f\"{class_label}_vrs_all\")\n",
    "    y_train_path = os.path.join(onevrsall_path, f\"y_train_{class_label}.csv\")\n",
    "    y_train_class_vs_rest = pd.read_csv(y_train_path)\n",
    "    \n",
    "    # Dictionary to store the top features for the current class\n",
    "    top_features_per_class = {}\n",
    "    \n",
    "    # Trained a SVC for the one-vs-rest classifier\n",
    "    classifier = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classifier.fit(x_train_class_vs_rest, y_train_class_vs_rest)\n",
    "    \n",
    "    # Get the coefficients (weights) for the features\n",
    "    feature_weights = classifier.estimators_[0].coef_[0]\n",
    "    # Sort the features by their absolute weights and select the top features\n",
    "    for no_f in no_of_features_per_class:\n",
    "        top_feature_indices = np.argsort(np.abs(feature_weights))[::-1][:no_f]\n",
    "        top_features = x_train_class_vs_rest.columns[top_feature_indices]\n",
    "        top_features_per_class[no_f] = top_features.tolist()\n",
    "\n",
    "    top_features_per_cd[class_label] = top_features_per_class\n",
    "\n",
    "final_datasets = {}\n",
    "for i in range(10):\n",
    "    for no_f in no_of_features_per_class:\n",
    "        dataset_name = f\"Data set {no_f}({i})\"\n",
    "        \n",
    "        final_datasets[dataset_name] = pd.DataFrame(x_train_class_vs_rest[top_features_per_cd[i][no_f]])\n",
    "print(\"30 Datasets for each class and for each top features [5,10,20]: \\n\", final_datasets)\n",
    "\n",
    "final_3_datasets = {}\n",
    "for no_f in no_of_features_per_class:\n",
    "    dataset_name = f\"Data set {no_f}\"\n",
    "    \n",
    "    selected_features = []\n",
    "    for class_label, top_features in top_features_per_cd.items():\n",
    "        selected_features.extend(top_features[no_f])\n",
    "    \n",
    "    final_3_datasets[dataset_name] = x_train_class_vs_rest[selected_features]\n",
    "print(\"3 Datasets Combined: \\n\", final_3_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "seed_value = 24\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for i in final_datasets:\n",
    "    no_aug_y_train = np.asarray(no_aug_y_train).ravel()\n",
    "    x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(final_datasets[i], no_aug_y_train, test_size=0.3, random_state=seed_value)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train_split, y_train_split)\n",
    "\n",
    "    predictions_gb = gnb.predict(x_test_split)\n",
    "    \n",
    "    accuracy_gb = accuracy_score(y_test_split, predictions_gb)\n",
    "    accuracies.append(accuracy_gb)\n",
    "    print(f'Accuracy for {i} = ', accuracy_gb)\n",
    "accuracies = np.array(accuracies)\n",
    "print(\"Mean Accuracy = \", accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = final_3_datasets['Data set 5']\n",
    "dataset2 = final_3_datasets['Data set 10']\n",
    "dataset3 = final_3_datasets['Data set 20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd3c8e",
   "metadata": {},
   "source": [
    "### 6) Run Naïve Bayes Classifier on data sets created & Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Gaussian Naïve Bayes Classifier on the 30 datasets created for each class and for each top correlating features \n",
    "[5,10,20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "class_labels = list(range(10))\n",
    "\n",
    "def evaluate_guassian_nb(X_train, y_train):\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    precision = precision_score(y_train, y_pred, average='macro', zero_division=1)\n",
    "    recall = recall_score(y_train, y_pred, average='macro', zero_division=1)\n",
    "    f1 = f1_score(y_train, y_pred, average='macro', zero_division=1)\n",
    "    roc_auc = roc_auc_score(y_train, nb_model.predict_proba(X_train), multi_class='ovr')\n",
    "    cross_val_scores = cross_val_score(nb_model, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    return accuracy, precision, recall, f1, roc_auc, cross_val_scores\n",
    "\n",
    "y_train_dict = {}\n",
    "evaluation_metrics = []\n",
    "dataset_number_features = [5, 10, 20]\n",
    "\n",
    "for i in final_datasets:\n",
    "    X_train_fr = final_datasets[i]\n",
    "    \n",
    "    accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_guassian_nb(X_train_fr, no_aug_y_train)\n",
    "\n",
    "    evaluation_metrics.append({\n",
    "        \"Dataset\": i,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Cross Val Scores\": np.mean(cross_val_scores),\n",
    "        \"TP rate\": recall, \n",
    "        \"FP rate\": 1 - recall\n",
    "    })\n",
    "\n",
    "df_evaluation_metrics = pd.DataFrame(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Gaussian Naïve Bayes Classifier on the final 3 combined datasets created of each class [5,10,20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "class_labels = list(range(10))\n",
    "\n",
    "def evaluate_gaussian_nb(X_train, y_train):\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    precision = precision_score(y_train, y_pred)\n",
    "    recall = recall_score(y_train, y_pred)\n",
    "    f1 = f1_score(y_train, y_pred)\n",
    "    roc_auc = roc_auc_score(y_train, y_pred)\n",
    "    cross_val_scores = cross_val_score(nb_model, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    return accuracy, precision, recall, f1, roc_auc, cross_val_scores\n",
    "\n",
    "datasets = [dataset1, dataset2, dataset3]  \n",
    "y_train_dict = {}\n",
    "evaluation_metrics = []\n",
    "dataset_number_features = [5, 10, 20]\n",
    "\n",
    "for dataset_number, dataset in enumerate(datasets):\n",
    "    X_train_fr = dataset\n",
    "    \n",
    "    for class_label in class_labels:\n",
    "        onevrsall_path = os.path.join(\"OnevrsAll\", f\"{class_label}_vrs_all\")\n",
    "        y_train_file = os.path.join(onevrsall_path, f\"y_train_{class_label}.csv\")\n",
    "        y_train_df = pd.read_csv(y_train_file).values.ravel()\n",
    "    \n",
    "        accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_gaussian_nb(X_train_fr, y_train_df)\n",
    "\n",
    "        evaluation_metrics.append({\n",
    "            \"Dataset\": str(dataset_number_features[dataset_number]),\n",
    "            \"Class Label\": class_label,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"Cross Val Scores\": np.mean(cross_val_scores),\n",
    "            \"TP rate\": recall, \n",
    "            \"FP rate\": 1 - recall\n",
    "        })\n",
    "\n",
    "df3_evaluation_metrics = pd.DataFrame(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(4, 4)) \n",
    "    for dataset_number in [5, 10, 20]:\n",
    "        dataset_metrics = df3_evaluation_metrics[df3_evaluation_metrics['Dataset'] == str(dataset_number)]\n",
    "        x_values = dataset_metrics['Class Label']\n",
    "        y_values = dataset_metrics[metric]\n",
    "        plt.plot(x_values, y_values, marker='o', label=f'Dataset {dataset_number}')\n",
    "\n",
    "    plt.title(f'{metric} by Class Label for Different Datasets')\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(x_values)  \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b32b7",
   "metadata": {},
   "source": [
    "### Explored the Coffee Leaf dataset that we used in Lab 4 and Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34674950",
   "metadata": {},
   "source": [
    "#### JMuBEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd405fa",
   "metadata": {},
   "source": [
    "### 1) Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "dataset_url = 'https://github.com/ishaqmarashy/DATALFS/raw/main/JMuBEN.zip'\n",
    "dataset_dir = './JMuBEN'\n",
    "\n",
    "# create directory for dataset if it does not exist\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    \n",
    "# append JMuBEN.zip to the end of the path (this is where we download the file to)\n",
    "zip_file_path = os.path.join(dataset_dir, 'JMuBEN.zip')\n",
    "\n",
    "\n",
    "# check if file is downloaded already\n",
    "if not os.path.exists(zip_file_path):\n",
    "    \n",
    "    # file is not downloaded so fetch the file\n",
    "    response = requests.get(dataset_url)\n",
    "    \n",
    "    # write file to storage which is recieved from the response\n",
    "    with open(zip_file_path, 'wb') as zip_file:\n",
    "        zip_file.write(response.content)\n",
    "        \n",
    "    # unzip to zip file path\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "# within the concat train and test to become ./JMuBEN/train and JMuBEN ./JMuBEN/test\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e676456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # get subdirectories Healthy and Miner\n",
    "    \n",
    "    for class_name in os.listdir(directory):\n",
    "        \n",
    "        # concat subdirectory to get full path\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        # assign labels using class subdirectory\n",
    "        # label is determined by filepath\n",
    "        label = 0 if class_name == 'Miner' else 1\n",
    "        \n",
    "        # append labels and image paths to labels and images respectively\n",
    "        for filename in os.listdir(class_dir):\n",
    "            images.append(os.path.join(class_dir, filename))\n",
    "            labels.append(label)\n",
    "            \n",
    "    return images, labels\n",
    "\n",
    "# load file directories and their labels\n",
    "train_images_dir, train_labels = load_images_and_labels(train_dir)\n",
    "test_images_dir, test_labels = load_images_and_labels(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of images and labels\n",
    "\n",
    "print(f\"Train images:{len(train_images_dir)}  Labels:{len(train_labels)}\")\n",
    "print(f\"Test images:{len(test_images_dir)}  Labels:{len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea18575",
   "metadata": {},
   "source": [
    "#### 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def load_grayscale_images(image_paths):\n",
    "    loaded_images = []\n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  #  grayscale\n",
    "        loaded_images.append(img)\n",
    "    return loaded_images\n",
    "\n",
    "def resize_images(images_to_resize):\n",
    "    resized_images = []\n",
    "    for img in images_to_resize:\n",
    "        resized_img = cv2.resize(img, (128, 128))  # resize\n",
    "        resized_images.append(resized_img)\n",
    "    return resized_images\n",
    "\n",
    "def normalize_image(images_to_normalize):\n",
    "    normalized_images = []\n",
    "    for img in images_to_normalize:\n",
    "        normalized_img = img / 255.0  # normalize\n",
    "        normalized_images.append(normalized_img)\n",
    "    return normalized_images\n",
    "\n",
    "image_pipeline = Pipeline(steps=[\n",
    "    ('load_grayscale_images', FunctionTransformer(load_grayscale_images)),\n",
    "    ('resize_images', FunctionTransformer(resize_images)),\n",
    "    ('normalize_image', FunctionTransformer(normalize_image))\n",
    "])\n",
    "\n",
    "\n",
    "train_images=image_pipeline.transform(train_images_dir)\n",
    "test_images=image_pipeline.transform(test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c11ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed9ead",
   "metadata": {},
   "source": [
    "#### 3) Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58309297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_image = train_images[3]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image, cmap='viridis')  \n",
    "plt.colorbar()\n",
    "plt.title('Heatmap of Pixel Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ff819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = train_images[3]\n",
    "pixel_values = image.reshape(-1)  # Flatten the image into a 1D array\n",
    "mean_pixel_value = np.mean(pixel_values)\n",
    "std_pixel_value = np.std(pixel_values)\n",
    "min_pixel_value = np.min(pixel_values)\n",
    "max_pixel_value = np.max(pixel_values)\n",
    "\n",
    " \n",
    "print(f\"Mean Pixel Value: {mean_pixel_value}\")\n",
    "print(f\"Standard Deviation of Pixel Values: {std_pixel_value}\")\n",
    "print(f\"Minimum Pixel Value: {min_pixel_value}\")\n",
    "print(f\"Maximum Pixel Value: {max_pixel_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculate class counts\n",
    "class_counts_train= Counter(train_labels)\n",
    "print(class_counts_train)\n",
    "\n",
    "class_counts_test = Counter(test_labels)\n",
    "print(class_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ada25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Histogram of class distribution\n",
    "plt.hist(train_labels, bins=[0, 1,2], align='left', rwidth=0.6)\n",
    "plt.xticks([0, 1], ['0', '1'])\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution for train dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37693af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Histogram of class distribution\n",
    "plt.hist(test_labels, bins=[0, 1,2], align='left', rwidth=0.6)\n",
    "plt.xticks([0, 1], ['0', '1'])\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution for test dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Get the current working directory as the root path\n",
    "root_path = os.path.join(os.getcwd(), \"JMuBEN\")\n",
    "\n",
    "# Initialize empty lists to store the information\n",
    "sizes = []\n",
    "resolutions = []\n",
    "color_distributions = []\n",
    "\n",
    "# Iterate over each image file in each subdirectory\n",
    "for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Load the image file using OpenCV\n",
    "            img_path = os.path.join(dirpath, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # Extract the size of the image\n",
    "            size = os.path.getsize(img_path)\n",
    "            sizes.append(size)\n",
    "\n",
    "            # Extract the resolution of the image\n",
    "            resolution = img.shape[:2]\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            # Extract the color distribution of the image\n",
    "            color_distribution = np.bincount(img.flatten(), minlength=256)\n",
    "            color_distributions.append(color_distribution)\n",
    "\n",
    "# Convert the lists to numpy arrays for easier manipulation\n",
    "sizes = np.array(sizes)\n",
    "resolutions = np.array(resolutions)\n",
    "color_distributions = np.array(color_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d583be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the mean color distribution across all images\n",
    "mean_color_distribution = np.mean(color_distributions, axis=0)\n",
    "\n",
    "# Plot a bar chart of the mean color distribution\n",
    "plt.bar(np.arange(256), mean_color_distribution)\n",
    "plt.title(\"Mean Color Distribution of the dataset\")\n",
    "plt.xlabel(\"Colour Value (Combined RGB)\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a histogram of the image sizes\n",
    "plt.hist(sizes)\n",
    "plt.title(\"Distribution of Image Sizes \")\n",
    "plt.xlabel(\"File Size (bytes)\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a scatter plot of the image resolutions\n",
    "plt.scatter(resolutions[:, 0], resolutions[:, 1])\n",
    "plt.title(\"Distribution of Image Resolutions\")\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Height (pixels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5ee6b",
   "metadata": {},
   "source": [
    "#### 4) Exploring Image Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa37c",
   "metadata": {},
   "source": [
    "##### · Canny Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03209870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canny Edge Detection\n",
    "\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#replace image path as needed \n",
    "img_path  = os.path.join(os.getcwd(), \"JMuBEN/train/Miner/1 (4471).jpg\")\n",
    "img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "edges = cv.Canny(img,100,200)\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc327e9e",
   "metadata": {},
   "source": [
    "##### · Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image_grayscale = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "image_normal = cv2.imread(img_path)\n",
    "\n",
    "#threshold to segment the image\n",
    "threshold_value = 130  \n",
    "ret, thresholded_image = cv2.threshold(image_grayscale, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "ret, thresholded_image_coloured = cv2.threshold(image_normal, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Display \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(131), plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132), plt.imshow(thresholded_image, cmap='gray')\n",
    "plt.title('Thresholded Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133), plt.imshow(thresholded_image_coloured, cmap='gray')\n",
    "plt.title('Thresholded Coloured Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9682b7",
   "metadata": {},
   "source": [
    "#### 5) Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ae5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "train_labels = train_labels.reshape(train_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8344fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing Gaussian Naive Bayes for classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf2 = GaussianNB()\n",
    "train_labels = train_labels.ravel()\n",
    "clf2.fit(train_images, train_labels)\n",
    "predictions2 = clf2.predict(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c58a84",
   "metadata": {},
   "source": [
    "#### 6) Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(train_labels, predictions2)\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e733b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = confusion_matrix(train_labels, predictions2)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True,  fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "tp = confusion_matrix[0,0]\n",
    "print(\"TP = \", tp)\n",
    "fp = confusion_matrix[1,0]\n",
    "print(\"FP = \", fp)\n",
    "tn = confusion_matrix[1,1]\n",
    "print(\"TN = \", tn)\n",
    "fn = confusion_matrix[0,1]\n",
    "print(\"FN = \", fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity = ', sensitivity)\n",
    "specificity = tn/ (fp + tn)\n",
    "print('Specificity = ', specificity)\n",
    "precision = precision_score(train_labels, predictions2)\n",
    "print('Precision = ', precision)\n",
    "recall = recall_score(train_labels, predictions2)\n",
    "print('Recall = ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(train_labels, predictions2)\n",
    "average_precision = average_precision_score(train_labels, predictions2)\n",
    "plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740290e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate predicted probabilities for the positive class\n",
    "probabilities = clf2.predict_proba(train_images)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(train_labels, probabilities)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area Under Curve = \", roc_auc)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f84cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(clf2, train_images, train_labels, scoring=\"accuracy\", cv=10)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores2)\n",
    "    print(\"Mean:\", scores2.mean())\n",
    "    print(\"Standard deviation:\", scores2.std())\n",
    "\n",
    "display_scores(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = cross_val_score(clf2, train_images, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "clf2_rmse_scores = np.sqrt(-scores2)\n",
    "\n",
    "display_scores(clf2_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(train_labels, predictions2)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix.diagonal()\n",
    "fn = np.sum(confusion_matrix, axis=1) - tp\n",
    "fp = np.sum(confusion_matrix, axis=0) - tp\n",
    "tn = np.sum(confusion_matrix) - (tp + fn + fp)\n",
    "\n",
    "# TP Rate (Sensitivity or Recall)\n",
    "tp_rate = tp / (tp + fn)\n",
    "\n",
    "# FP Rate\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "for class_label, tp, fp in zip(range(len(tp_rate)), tp_rate, fp_rate):\n",
    "    print(f\"Class {class_label}: \\nTP Rate = {tp}, \\nFP Rate = {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee42993",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = []\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    true_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :]) - np.sum(confusion_matrix[:, i]) + confusion_matrix[i, i]\n",
    "    total_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :])\n",
    "    print(f'Label {unique_labels[i]} specificity: {true_negative / total_negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensitivity = recall_score(train_labels, predictions2, average=None)\n",
    "for i in range(len(unique_labels)):\n",
    "    print(f'Label {unique_labels[i]} sensitivity: {sensitivity[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting ROC curve for each class\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probabilities = clf2.predict_proba(train_images)\n",
    "roc_auc_scores = []\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    fpr, tpr, _ = roc_curve((train_labels == unique_labels[i]).astype(int), probabilities[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implemented PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "  \n",
    "# Reduce to 5 features\n",
    "num_components = 5 \n",
    "pca = PCA(num_components)\n",
    "train_reduced5 = pca.fit_transform(train_images)\n",
    "# Reduce to 10 features\n",
    "num_components = 10 \n",
    "pca = PCA(num_components)\n",
    "train_reduced10 = pca.fit_transform(train_images)\n",
    "# Reduce to 20 features\n",
    "num_components = 20\n",
    "pca = PCA(num_components)\n",
    "train_reduced20 = pca.fit_transform(train_images)\n",
    "\n",
    "train_labels = pd.DataFrame(train_labels)\n",
    "\n",
    "result5 = pd.concat([train_labels, pd.DataFrame(train_reduced5,columns=['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'])], axis=1)\n",
    "print(result5)\n",
    "result10 = pd.concat([train_labels, pd.DataFrame(train_reduced10,columns=['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10'])], axis=1)\n",
    "print(result10)\n",
    "result20 = pd.concat([train_labels, pd.DataFrame(train_reduced20,columns=['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13', 'PCA14', 'PCA15', 'PCA16', 'PCA17', 'PCA18', 'PCA19', 'PCA20'])], axis=1)\n",
    "print(result20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def evaluate_gaussian_nb(X_train, y_train):\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    precision = precision_score(y_train, y_pred)\n",
    "    recall = recall_score(y_train, y_pred)\n",
    "    f1 = f1_score(y_train, y_pred)\n",
    "    roc_auc = roc_auc_score(y_train, y_pred)\n",
    "    cross_val_scores = cross_val_score(nb_model, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    return accuracy, precision, recall, f1, roc_auc, cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_gaussian_nb(result5[['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5']], result5[0])\n",
    "accuracies.append(accuracy)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)\n",
    "print(\"ROC AUC = \", roc_auc)\n",
    "print(\"Cross Validation Score = \", cross_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_gaussian_nb(result10[['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10']], result10[0])\n",
    "accuracies.append(accuracy)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)\n",
    "print(\"ROC AUC = \", roc_auc)\n",
    "print(\"Cross Validation Score = \", cross_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_gaussian_nb(result20[['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13', 'PCA14', 'PCA15', 'PCA16', 'PCA17', 'PCA18', 'PCA19', 'PCA20']], result20[0])\n",
    "accuracies.append(accuracy)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)\n",
    "print(\"ROC AUC = \", roc_auc)\n",
    "print(\"Cross Validation Score = \", cross_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4acbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Accuracy = ', np.mean(accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
