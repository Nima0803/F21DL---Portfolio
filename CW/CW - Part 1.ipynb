{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364c78a3",
   "metadata": {},
   "source": [
    "## Part 1 - Data Analysis and Bayes Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6ebcf",
   "metadata": {},
   "source": [
    "### 1) Data Visualization and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d441226",
   "metadata": {},
   "source": [
    "Importing the required libraries and checking for the python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db10db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d7979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2  # OpenCV Python library for computer vision\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa1ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02f8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bb604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6070c37",
   "metadata": {},
   "source": [
    "Loading only the training set for Part 1 of the coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_DATASET_PATH = \"CW_dataset\"\n",
    "\n",
    "def load_train_data(dataset_path=CW_DATASET_PATH):\n",
    "    x_train_all_path = os.path.join(dataset_path, \"x_train_all.csv\")\n",
    "    y_train_all_path = os.path.join(dataset_path, \"y_train_all.csv\")\n",
    "    x_test_all_path = os.path.join(dataset_path, \"x_test_all.csv\")\n",
    "    y_test_all_path = os.path.join(dataset_path, \"y_test_all.csv\")\n",
    "\n",
    "    x_train_all = pd.read_csv(x_train_all_path)\n",
    "    y_train_all = pd.read_csv(y_train_all_path)\n",
    "    x_test_all = pd.read_csv(x_test_all_path)\n",
    "    y_test_all = pd.read_csv(y_test_all_path)\n",
    "\n",
    "    return x_train_all, y_train_all, x_test_all,y_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892a4d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CW_dataset/x_test_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW - Part 1.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train, y_train, x_test, y_test \u001b[39m=\u001b[39m load_train_data()\n",
      "\u001b[1;32m/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW - Part 1.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x_train_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(x_train_all_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_train_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(y_train_all_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m x_test_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(x_test_all_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y_test_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(y_test_all_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y145sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x_train_all, y_train_all, x_test_all,y_test_all\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CW_dataset/x_test_all.csv'"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f27ece",
   "metadata": {},
   "source": [
    "Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecaa628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the data\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the first 5 rows of the dataset\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "x_train.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if there are any missing values in the data frame\n",
    "x_train.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aada93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataset using describe\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = y_train['0'].value_counts().sort_index()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8ad66",
   "metadata": {},
   "source": [
    "Visualising the dataset using graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630e4a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='0', data=y_train)\n",
    "plt.title(\"Distribution of Class Labels\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77802396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display one image for each label\n",
    "\n",
    "# Initialize a dictionary to store one image for each label\n",
    "label_images = {}\n",
    "\n",
    "# Iterate through the rows of the DataFrames and find one image for each label\n",
    "for index, row in x_train.iterrows():\n",
    "    label = y_train.iloc[index, 0]  \n",
    "    \n",
    "    if label not in label_images:\n",
    "        # Store the first image for each unique label\n",
    "        label_images[label] = row.values.reshape(48, 48)  \n",
    "    \n",
    "    # Break the loop if we have found one image for each unique label\n",
    "    if len(label_images) == 10:\n",
    "        break\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, (label, image) in enumerate(label_images.items()):\n",
    "    r, c = divmod(i, 5)\n",
    "    axs[r, c].imshow(image)\n",
    "    axs[r, c].set_title(f'Label: {label}')\n",
    "    axs[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155af4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_class_labels = y_train['0'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(unique_class_labels), figsize=(20, 5))\n",
    "\n",
    "for i, class_label in enumerate(unique_class_labels):\n",
    "    # Select a representative image for each class\n",
    "    class_images = x_train[y_train['0'] == class_label]\n",
    "    \n",
    "    if not class_images.empty:\n",
    "        representative_image = class_images.iloc[0, :-1].values\n",
    "        \n",
    "        # Plot the histogram for the representative image of each class\n",
    "        axes[i].hist(representative_image, bins=100)\n",
    "        axes[i].set_title(f'Class {class_label}')\n",
    "        axes[i].set_xlabel(\"Pixel Value\")\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW - Part 1.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y411sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     num_images_per_label[label] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(label_images[label])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y411sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# get numpy array from dictionary values (image data)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y411sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m box_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(label_images\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y411sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# labels from keys\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fayazbadubhai/GitHub_Projects/DMML-GroupProject/CW/CW%20-%20Part%201.ipynb#Y411sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(label_images\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# to be fixed\n",
    "\n",
    "# # store image data for each label\n",
    "# label_images = {}\n",
    "\n",
    "# # just to check if all images are being plotted\n",
    "# num_images_per_label = {}\n",
    "\n",
    "# for index, row in x_train.iterrows():\n",
    "#     label = y_train.iloc[index, 0]  \n",
    "#     image_data = row.values.reshape(-1, 48, 48) \n",
    "\n",
    "#     if label not in label_images:\n",
    "#         label_images[label] = []\n",
    "    \n",
    "#     # add image data to associated labels\n",
    "#     label_images[label].append(image_data)\n",
    "\n",
    "#     # just to check if all images are being plotted\n",
    "#     num_images_per_label[label] = len(label_images[label])\n",
    "\n",
    "# # get numpy array from dictionary values (image data)\n",
    "# box_data = np.array(list(label_images.values()))\n",
    "\n",
    "# # labels from keys\n",
    "# labels = list(label_images.keys())\n",
    "\n",
    "# # box plots for each label \n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.boxplot(box_data, labels=labels, vert=False)\n",
    "# plt.title(\"Box Plots for each label\")\n",
    "# plt.xlabel(\"Pixel Values\")\n",
    "# plt.ylabel(\"Label\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # just to check if all images are being plotted\n",
    "# for label, num_images in num_images_per_label.items():\n",
    "#     print(f\"Label {label}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f6f06",
   "metadata": {},
   "source": [
    "### 2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f00bf",
   "metadata": {},
   "source": [
    "To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a57026",
   "metadata": {},
   "source": [
    "### 3) Running Naïve Bayes Classifier on Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa64e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "train_images = x_train.reshape(x_train.shape[0], -1)\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_images, y_train)\n",
    "\n",
    "predictions = clf.predict(x_train)\n",
    "\n",
    "\"\"\"\n",
    "#Using the stratified train test split to split the data into train and test sets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_images,y_train,test_size=0.33, random_state=42)\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train, Y_train)\n",
    "predictions2 = clf2.predict(X_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab84d5",
   "metadata": {},
   "source": [
    "### 4) Evaluation Metrics for the Naïve Bayes Classifier on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ac8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix.diagonal()\n",
    "fn = np.sum(confusion_matrix, axis=1) - tp\n",
    "fp = np.sum(confusion_matrix, axis=0) - tp\n",
    "tn = np.sum(confusion_matrix) - (tp + fn + fp)\n",
    "\n",
    "tp_rate = tp / (tp + fn)\n",
    "\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "for class_label, tp, fp in zip(range(len(tp_rate)), tp_rate, fp_rate):\n",
    "    print(f\"Class {class_label}: \\nTP Rate = {tp}, \\nFP Rate = {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588110e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = []\n",
    "unique_labels = np.unique(y_test)\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    true_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :]) - np.sum(confusion_matrix[:, i]) + confusion_matrix[i, i]\n",
    "    total_negative = np.sum(confusion_matrix) - np.sum(confusion_matrix[i, :])\n",
    "    print(f'Label {unique_labels[i]} specificity: {true_negative / total_negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensitivity = recall_score(y_test, predictions, average=None)\n",
    "for i in range(len(unique_labels)):\n",
    "    print(f'Label {unique_labels[i]} sensitivity: {sensitivity[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a636d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_roc_curves(probabilities,  y_test, unique_labels):\n",
    "    roc_auc_scores = []\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in range(len(unique_labels)):\n",
    "        fpr, tpr, _ = roc_curve((y_test == unique_labels[i]).astype(int), probabilities[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "probabilities = clf.predict_proba(x_test)\n",
    "plot_roc_curves(probabilities, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdf51d",
   "metadata": {},
   "source": [
    "#### Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, x_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb528fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, x_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "clf_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "display_scores(clf_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b157dc",
   "metadata": {},
   "source": [
    "### 5) Top Correlating Features - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbb3ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fayazbadubhai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "no_of_features_per_class = [5, 10, 20]\n",
    "top_features_per_cd = {}\n",
    "\n",
    "x_train_class_vs_rest = pd.DataFrame(x_train)\n",
    "\n",
    "for class_label in range(10):  \n",
    "    onevrsall_path = os.path.join(\"OnevrsAll\", f\"{class_label}_vrs_all\")\n",
    "    y_train_path = os.path.join(onevrsall_path, f\"y_train_{class_label}.csv\")\n",
    "    y_train_class_vs_rest = pd.read_csv(y_train_path)\n",
    "    \n",
    "    top_features_per_class = {}\n",
    "    # Train a logistic regression classifier for the current one-vs-rest classification task\n",
    "    classifier = OneVsRestClassifier(LogisticRegression())\n",
    "    classifier.fit(x_train_class_vs_rest, y_train_class_vs_rest)\n",
    "    \n",
    "    # Get the coefficients (weights) for the features\n",
    "    feature_weights = classifier.estimators_[0].coef_[0]\n",
    "    # Sort the features by their absolute weights and select the top features\n",
    "    for no_f in no_of_features_per_class:\n",
    "        top_feature_indices = np.argsort(np.abs(feature_weights))[::-1][:no_f]\n",
    "        top_features = x_train_class_vs_rest.columns[top_feature_indices]\n",
    "        top_features_per_class[no_f] = top_features.tolist()\n",
    "    \n",
    "    # Store the top features for the current class\n",
    "    top_features_per_cd[class_label] = top_features_per_class\n",
    "\n",
    "# Create the final datasets with selected features\n",
    "final_datasets = {}\n",
    "for no_f in no_of_features_per_class:\n",
    "    dataset_name = f\"Data set {no_f}\"\n",
    "    \n",
    "    # Combine selected features for all classes\n",
    "    selected_features = []\n",
    "    for class_label, top_features in top_features_per_cd.items():\n",
    "        selected_features.extend(top_features[no_f])\n",
    "    \n",
    "    # Create the final dataset with selected features\n",
    "    final_datasets[dataset_name] = x_train_class_vs_rest[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b45d16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = final_datasets['Data set 5']\n",
    "dataset2 = final_datasets['Data set 10']\n",
    "dataset3 = final_datasets['Data set 20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05794b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "class_labels = list(range(10))\n",
    "\n",
    "def evaluate_multinomial_nb(X_train, X_test, y_train, y_test):\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cross_val_scores = cross_val_score(nb_model, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    return accuracy, precision, recall, f1, roc_auc, cross_val_scores\n",
    "\n",
    "datasets = [dataset1, dataset2, dataset3]  \n",
    "y_train_dict = {}\n",
    "y_test_dict = {}\n",
    "evaluation_metrics = []\n",
    "dataset_number_features = [5, 10, 20]\n",
    "\n",
    "for dataset_number, dataset in enumerate(datasets):\n",
    "    X_train_fr = dataset\n",
    "    x_test_int = x_test.astype('int64')\n",
    "    x_test_int.columns = x_test_int.columns.astype('int64')\n",
    "    X_test_fr = pd.DataFrame(x_test_int[dataset.columns], columns=dataset.columns)\n",
    "    \n",
    "    for class_label in class_labels:\n",
    "        onevrsall_path = os.path.join(\"OnevrsAll\", f\"{class_label}_vrs_all\")\n",
    "        y_train_file = os.path.join(onevrsall_path, f\"y_train_{class_label}.csv\")\n",
    "        y_test_file = os.path.join(onevrsall_path, f\"y_test_{class_label}.csv\")\n",
    "        y_train_df = pd.read_csv(y_train_file).values.ravel()\n",
    "        y_test_df = pd.read_csv(y_test_file).values.ravel()\n",
    "    \n",
    "        accuracy, precision, recall, f1, roc_auc, cross_val_scores = evaluate_multinomial_nb(X_train_fr, X_test_fr, y_train_df, y_test_df)\n",
    "\n",
    "        evaluation_metrics.append({\n",
    "            \"Dataset\": str(dataset_number_features[dataset_number]),\n",
    "            \"Class Label\": class_label,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"Cross Val Scores\": np.mean(cross_val_scores) \n",
    "        })\n",
    "\n",
    "df_evaluation_metrics = pd.DataFrame(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6960c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(4, 4)) \n",
    "    for dataset_number in [5, 10, 20]:\n",
    "        dataset_metrics = df_evaluation_metrics[df_evaluation_metrics['Dataset'] == str(dataset_number)]\n",
    "        x_values = dataset_metrics['Class Label']\n",
    "        y_values = dataset_metrics[metric]\n",
    "        plt.plot(x_values, y_values, marker='o', label=f'Dataset {dataset_number}')\n",
    "\n",
    "    plt.title(f'{metric} by Class Label for Different Datasets')\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(x_values)  \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee775ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
