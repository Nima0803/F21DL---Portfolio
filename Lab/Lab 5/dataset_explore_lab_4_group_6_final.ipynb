{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JMuBEN2 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://data.mendeley.com/datasets/tgv3zb82nd/1)\n",
    "\n",
    "Image dataset taken from an Arabica coffee plantation. The image dataset contains collection of healthy and miner coffee leaves.\n",
    "\n",
    "We used the same dataset that we used in Lab 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Spliting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wNVZNNGPpUsJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "dataset_url = 'https://github.com/ishaqmarashy/DATALFS/raw/main/JMuBEN.zip'\n",
    "dataset_dir = './JMuBEN'\n",
    "\n",
    "# create directory for dataset if it does not exist\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    \n",
    "# append JMuBEN.zip to the end of the path (this is where we download the file to)\n",
    "zip_file_path = os.path.join(dataset_dir, 'JMuBEN.zip')\n",
    "\n",
    "\n",
    "# check if file is downloaded already\n",
    "if not os.path.exists(zip_file_path):\n",
    "    \n",
    "    # file is not downloaded so fetch the file\n",
    "    response = requests.get(dataset_url)\n",
    "    \n",
    "    # write file to storage which is recieved from the response\n",
    "    with open(zip_file_path, 'wb') as zip_file:\n",
    "        zip_file.write(response.content)\n",
    "        \n",
    "    # unzip to zip file path\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "# within the concat train and test to become ./JMuBEN/train and JMuBEN ./JMuBEN/test\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wNVZNNGPpUsJ"
   },
   "outputs": [],
   "source": [
    "def load_images_and_labels(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # get subdirectories Healthy and Miner\n",
    "    \n",
    "    for class_name in os.listdir(directory):\n",
    "        \n",
    "        # concat subdirectory to get full path\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        # assign labels using class subdirectory\n",
    "        # label is determined by filepath\n",
    "        label = 0 if class_name == 'Miner' else 1\n",
    "        \n",
    "        # append labels and image paths to labels and images respectively\n",
    "        for filename in os.listdir(class_dir):\n",
    "            images.append(os.path.join(class_dir, filename))\n",
    "            labels.append(label)\n",
    "            \n",
    "    return images, labels\n",
    "\n",
    "# load file directories and their labels\n",
    "train_images_dir, train_labels = load_images_and_labels(train_dir)\n",
    "test_images_dir, test_labels = load_images_and_labels(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wWJu4XIpUsK",
    "outputId": "b5dc0d20-b7a1-4b1b-a1b2-dfe19da8c1ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images:24000  Labels:24000\n",
      "Test images:6000  Labels:6000\n"
     ]
    }
   ],
   "source": [
    "# print the number of images and labels\n",
    "\n",
    "print(f\"Train images:{len(train_images_dir)}  Labels:{len(train_labels)}\")\n",
    "print(f\"Test images:{len(test_images_dir)}  Labels:{len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images in the dataset were converted to grayscale and resized to 128x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried loading as described https://www.analyticsvidhya.com/blog/2021/07/step-by-step-guide-for-image-classification-on-custom-datasets/\n",
    "\n",
    "https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6lxOlSPGrUn8"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def load_grayscale_images(image_paths):\n",
    "    loaded_images = []\n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  #  grayscale\n",
    "        loaded_images.append(img)\n",
    "    return loaded_images\n",
    "\n",
    "def resize_images(images_to_resize):\n",
    "    resized_images = []\n",
    "    for img in images_to_resize:\n",
    "        resized_img = cv2.resize(img, (128, 128))  # resize\n",
    "        resized_images.append(resized_img)\n",
    "    return resized_images\n",
    "\n",
    "def normalize_image(images_to_normalize):\n",
    "    normalized_images = []\n",
    "    for img in images_to_normalize:\n",
    "        normalized_img = img / 255.0  # normalize\n",
    "        normalized_images.append(normalized_img)\n",
    "    return normalized_images\n",
    "\n",
    "image_pipeline = Pipeline(steps=[\n",
    "    ('load_grayscale_images', FunctionTransformer(load_grayscale_images)),\n",
    "    ('resize_images', FunctionTransformer(resize_images)),\n",
    "    ('normalize_image', FunctionTransformer(normalize_image))\n",
    "])\n",
    "\n",
    "\n",
    "train_images=image_pipeline.transform(train_images_dir)\n",
    "test_images=image_pipeline.transform(test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "train_labels = train_labels.reshape(train_images.shape[0], -1)\n",
    "test_images = test_images.reshape(test_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1 = MultinomialNB(alpha=1.0e-10, force_alpha=True , class_prior=[0.4, 0.6])\n",
    "train_labels = train_labels.ravel()\n",
    "clf1.fit(train_images, train_labels)\n",
    "clf1.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(train_images, train_labels)\n",
    "clf2.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "clf3 = ComplementNB(force_alpha=True)\n",
    "clf3.fit(train_images, train_labels)\n",
    "clf3.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf4 = BernoulliNB(force_alpha=True)\n",
    "clf4.fit(train_images, train_labels)\n",
    "clf4.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf5 = CategoricalNB(force_alpha=True)\n",
    "clf5.fit(train_images, train_labels)\n",
    "clf5.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Complex Bayes net (to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.array(test_images)\n",
    "test_images = test_images.reshape(-1, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate how well the model did to predict the labels on the test images\n",
    "score = model.evaluate(test_images, np.array(test_labels))\n",
    "print(f'Test loss: {score[0]}/Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign classes for high probabilities\n",
    "predict_df = pd.DataFrame(predict)\n",
    "class_predictions = predict_df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_labels, class_predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, class_predictions)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(test_labels, class_predictions)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precision, recall, f1-score values \n",
    "print(classification_report(test_labels, class_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, predict[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                         # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sgd_clf.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = confusion_matrix(test_labels, predictions)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix[0,0]\n",
    "tn = confusion_matrix[1,1]\n",
    "fp = confusion_matrix[1,0]\n",
    "fn = confusion_matrix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_rate = tp / (tp + fn)\n",
    "tp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_rate = fp / (fp + tn)\n",
    "fp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_2 = precision_score(test_labels, predictions)\n",
    "precision_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_2 = recall_score(test_labels, predictions)\n",
    "recall_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, sgd_clf.decision_function(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                         # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
